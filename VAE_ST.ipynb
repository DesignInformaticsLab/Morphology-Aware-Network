{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incorporate point-correlation\n",
    "def correlation_fun(x,input_dim,Rad):\n",
    "    point_corr=0\n",
    "    for i in range(input_dim):\n",
    "        for j in range(input_dim):\n",
    "            point_corr_temp1=tf.multiply(x[0][i,j],x[0][i,tf.mod(j+Rad,input_dim)])\n",
    "            point_corr=tf.add(point_corr_temp1,point_corr)\n",
    "    \n",
    "    for i in range(input_dim):\n",
    "        for j in range(input_dim):           \n",
    "            point_corr_temp2=tf.multiply(x[0][i,j],x[0][tf.mod(i+Rad,input_dim),j])\n",
    "            point_corr=tf.add(point_corr_temp2,point_corr)\n",
    "    return (point_corr+2*input_dim**2)/4.\n",
    "\n",
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(128, 128), cmap='Greys_r')\n",
    "\n",
    "    return fig\n",
    "\n",
    "def conv2d(x, W, stride, padding=\"SAME\"):\n",
    "        return tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding=padding)\n",
    "    \n",
    "def max_pool(x, k_size, stride, padding=\"SAME\"):\n",
    "    # use avg pooling instead, as described in the paper\n",
    "    return tf.nn.avg_pool(x, ksize=[1, k_size, k_size, 1], \n",
    "            strides=[1, stride, stride, 1], padding=padding)       \n",
    "    \n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)\n",
    "\n",
    "\n",
    "def sample_z(mu, log_var):\n",
    "    eps = tf.random_normal(shape=tf.shape(mu),stddev=1.0)\n",
    "    return mu + tf.exp(log_var / 2) * eps\n",
    "\n",
    "def gram_matrix(x):\n",
    "    features = backend.batch_flatten(backend.permute_dimensions(x, (2, 0, 1)))\n",
    "#     features_mean,features_var =tf.nn.moments(features,axes=[0])    \n",
    "    features_mean = tf.reduce_mean(features,0)\n",
    "    features = (features-features_mean)/1\n",
    "    gram = backend.dot(features, backend.transpose(features))  \n",
    "    return gram\n",
    "\n",
    "def style_loss(style, combination):\n",
    "    loss_temp=0.\n",
    "    channels = 3\n",
    "    size = height * width\n",
    "    \n",
    "    for i in range(mb_size):\n",
    "        C = gram_matrix(combination[i])\n",
    "        S = gram_matrix(style[i])\n",
    "        loss_temp = tf.add(loss_temp,backend.sum(backend.square(S - C))/(4. * (channels ** 2) * (size ** 2)))*1e0\n",
    "    \n",
    "    return loss_temp\n",
    "\n",
    "def claps_loss(x):\n",
    "    z_d_gen=backend.batch_flatten(x)          \n",
    "    nom = tf.matmul(z_d_gen, tf.transpose(z_d_gen, perm=[1, 0]))\n",
    "    denom = tf.sqrt(tf.reduce_sum(tf.square(z_d_gen), reduction_indices=[1], keep_dims=True))\n",
    "    pt = tf.square(tf.transpose((nom / denom), (1, 0)) / denom)\n",
    "    pt = pt - tf.diag(tf.diag_part(pt))\n",
    "    pulling_term = tf.reduce_sum(pt) / (mb_size * (mb_size - 1))*10\n",
    "    \n",
    "    return pulling_term\n",
    "\n",
    "def P(z):\n",
    "    h1 = tf.nn.relu(tf.matmul(z, P_W1) + P_b1)\n",
    "    h2_1 = tf.nn.relu(tf.add(tf.nn.conv2d_transpose(tf.reshape(h1,[mb_size, width/8, height/8, 1]), \n",
    "                                                  deconv2_1_weight, strides=[1, 1, 1, 1], padding='SAME',\n",
    "                                       output_shape=[mb_size, width/8, height/8, deconv2_1_features]),deconv2_1_bias))\n",
    "\n",
    "    h2_2 = tf.nn.relu(tf.add(tf.nn.conv2d_transpose(h2_1,deconv2_2_weight, strides=[1, 2, 2, 1], padding='SAME',\n",
    "                                       output_shape=[mb_size, width/4, height/4, deconv2_2_features]),deconv2_2_bias))    \n",
    "    \n",
    "    h3_1 = tf.nn.relu(tf.add(tf.nn.conv2d_transpose(h2_2, deconv3_1_weight, strides=[1, 1, 1, 1], padding='SAME',\n",
    "                                       output_shape=[mb_size, width/4, height/4, deconv3_1_features]),deconv3_1_bias))\n",
    "\n",
    "    h3_2 = tf.nn.relu(tf.add(tf.nn.conv2d_transpose(h3_1, deconv3_2_weight, strides=[1, 2, 2, 1], padding='SAME',\n",
    "                                       output_shape=[mb_size, width/2, height/2, deconv3_2_features]),deconv3_2_bias))\n",
    "    \n",
    "    h4_1 = tf.nn.relu(tf.add(tf.nn.conv2d_transpose(h3_2, deconv4_1_weight, strides=[1, 1, 1, 1], padding='SAME',\n",
    "                                       output_shape=[mb_size, width/2, height/2, deconv4_1_features]),deconv4_1_bias))\n",
    "\n",
    "    h4_2 = tf.nn.relu(tf.add(tf.nn.conv2d_transpose(h4_1, deconv4_2_weight, strides=[1, 2, 2, 1], padding='SAME',\n",
    "                                       output_shape=[mb_size, width/1, height/1, deconv4_2_features]),deconv4_2_bias))    \n",
    "    \n",
    "    h5 = (tf.add(tf.nn.conv2d_transpose(h4_2, deconv5_weight, strides=[1, 1, 1, 1], padding='SAME',\n",
    "                                        output_shape=[mb_size, width/1, height/1, 1]),deconv5_bias))\n",
    "    \n",
    "#     prob = tf.nn.sigmoid(h5)\n",
    "    prob = 1 / (1 + tf.exp(-h5))\n",
    "    \n",
    "    return prob\n",
    "\n",
    "\n",
    "def Q(X):\n",
    "    h1_1 = tf.nn.relu(tf.add(conv2d(tf.reshape(X,[mb_size, width/1, height/1, 1]),\n",
    "                                  conv1_1_weight, stride=1, padding='SAME'),conv1_1_bias))\n",
    "    \n",
    "    h1_2 = tf.nn.relu(tf.add(conv2d(h1_1,conv1_2_weight, stride=2, padding='SAME'),conv1_2_bias))\n",
    "    \n",
    "    h2_1 = tf.nn.relu(tf.add(conv2d(h1_2,conv2_1_weight, stride=1, padding='SAME'),conv2_1_bias))\n",
    "    \n",
    "    h2_2 = tf.nn.relu(tf.add(conv2d(h2_1,conv2_2_weight, stride=2, padding='SAME'),conv2_2_bias))\n",
    "\n",
    "    h3_1 = tf.nn.relu(tf.add(conv2d(h2_2,conv3_1_weight, stride=1, padding='SAME'),conv3_1_bias))\n",
    "    \n",
    "    h3_2 = tf.nn.relu(tf.add(conv2d(h3_1,conv3_2_weight, stride=2, padding='SAME'),conv3_2_bias))    \n",
    "    \n",
    "    h4 = tf.nn.relu(tf.matmul(tf.reshape(h3_2,[mb_size,width/8*height/8*conv3_2_features]), Q_W1) + Q_b1)\n",
    "    \n",
    "    z_mu = (tf.matmul(h4, Q_W2_mu) + Q_b2_mu)\n",
    "    z_logvar = (tf.matmul(h4, Q_W2_sigma) + Q_b2_sigma)\n",
    "    \n",
    "    return z_mu, z_logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## loading input data ########\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "import scipy.io as sio\n",
    "import h5py\n",
    "from keras import backend\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "# images = abs(1-np.array(sio.loadmat('alloy_mat/WB_128_all_angle.mat')['WB'],dtype='float32'))\n",
    "# images_L = np.array(sio.loadmat('alloy_mat/sandstone_v2_Young.mat')['L'],dtype='float32')\n",
    "\n",
    "# images = (np.array(sio.loadmat('alloy_mat/sandstone_v2.mat')['Data'],dtype='float32'))\n",
    "# images_L = np.array(sio.loadmat('alloy_mat/sandstone_v2_Young.mat')['L'],dtype='float32')\n",
    "\n",
    "# images = np.array(sio.loadmat('alloy_mat/diffusion_coefficient_train.mat')['Data'],dtype='float32')\n",
    "# images_L = np.array(sio.loadmat('alloy_mat/diffusion_coefficient_train.mat')['L'],dtype='float32')\n",
    "\n",
    "images = np.array(sio.loadmat('alloy_mat/diffusion_coefficient_train_large.mat')['WB_large'],dtype='float32')\n",
    "\n",
    "images=images[0:200]\n",
    "# images_L=images_L[0:200]\n",
    "\n",
    "mb_size = 10\n",
    "X_dim = images.shape[1]\n",
    "width = 128*2\n",
    "height = 128*2\n",
    "h_dim = width/8*height/8\n",
    "z_dim = 128*2/8\n",
    "\n",
    "\n",
    "conv1_1_features=32\n",
    "conv1_2_features=32\n",
    "conv2_1_features=32*2\n",
    "conv2_2_features=32*2\n",
    "conv3_1_features=32*3\n",
    "conv3_2_features=32*3\n",
    "conv4_features=1\n",
    "c = 0\n",
    "\n",
    "deconv2_1_features=32*3\n",
    "deconv2_2_features=32*3\n",
    "deconv3_1_features=32*2\n",
    "deconv3_2_features=32*2\n",
    "deconv4_1_features=32\n",
    "deconv4_2_features=32\n",
    "\n",
    "# images_style = np.array(sio.loadmat('WB_raw2.mat')['WB'],dtype='float32')\n",
    "\n",
    "style_array = np.zeros([len(images),height,width,3])\n",
    "style_array[:,:,:,0]=(images.reshape(len(images),height,width))*1\n",
    "style_array[:,:,:,1]=(images.reshape(len(images),height,width))*1\n",
    "style_array[:,:,:,2]=(images.reshape(len(images),height,width))*1\n",
    "print(style_array.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## weights & bias loaading ########\n",
    "f = h5py.File('weights_VGG/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5','r')\n",
    "ks = f.keys()\n",
    "\n",
    "vgg16_weights=[]\n",
    "vgg16_bias=[]\n",
    "for i in range(18):\n",
    "    if (len(f[ks[i]].values())) != 0:        \n",
    "        vgg16_weights.append(f[ks[i]].values()[0][:])\n",
    "        vgg16_bias.append(f[ks[i]].values()[1][:])\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "W_conv1 = (tf.constant(vgg16_weights[0]))\n",
    "W_conv2 = (tf.constant(vgg16_weights[1]))\n",
    "W_conv3 = (tf.constant(vgg16_weights[2]))\n",
    "W_conv4 = (tf.constant(vgg16_weights[3]))\n",
    "W_conv5 = (tf.constant(vgg16_weights[4]))\n",
    "W_conv6 = (tf.constant(vgg16_weights[5]))\n",
    "W_conv7 = (tf.constant(vgg16_weights[6]))\n",
    "W_conv8 = (tf.constant(vgg16_weights[7]))\n",
    "W_conv9 = (tf.constant(vgg16_weights[8]))\n",
    "W_conv10= (tf.constant(vgg16_weights[9]))\n",
    "W_conv11= (tf.constant(vgg16_weights[10]))\n",
    "W_conv12= (tf.constant(vgg16_weights[11]))\n",
    "W_conv13= (tf.constant(vgg16_weights[12]))\n",
    "\n",
    "b_conv1 = tf.reshape(tf.constant(vgg16_bias[0]),[-1])\n",
    "b_conv2 = tf.reshape(tf.constant(vgg16_bias[1]),[-1])\n",
    "b_conv3 = tf.reshape(tf.constant(vgg16_bias[2]),[-1])\n",
    "b_conv4 = tf.reshape(tf.constant(vgg16_bias[3]),[-1])\n",
    "b_conv5 = tf.reshape(tf.constant(vgg16_bias[4]),[-1])\n",
    "b_conv6 = tf.reshape(tf.constant(vgg16_bias[5]),[-1])\n",
    "b_conv7 = tf.reshape(tf.constant(vgg16_bias[6]),[-1])\n",
    "b_conv8 = tf.reshape(tf.constant(vgg16_bias[7]),[-1])\n",
    "b_conv9 = tf.reshape(tf.constant(vgg16_bias[8]),[-1])\n",
    "b_conv10 = tf.reshape(tf.constant(vgg16_bias[9]),[-1])\n",
    "b_conv11 = tf.reshape(tf.constant(vgg16_bias[10]),[-1])\n",
    "b_conv12 = tf.reshape(tf.constant(vgg16_bias[11]),[-1])\n",
    "b_conv13 = tf.reshape(tf.constant(vgg16_bias[12]),[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================== Q(z|X) ======================================\n",
    "z = tf.placeholder(tf.float32, shape=[None, z_dim])\n",
    "x_input_shape = (mb_size, width, height, 1)\n",
    "X = tf.placeholder(tf.float32, shape=x_input_shape)\n",
    "kl_ratio=tf.placeholder(tf.float32,shape=())\n",
    "\n",
    "conv1_1_weight = tf.Variable(tf.truncated_normal([4, 4, 1, conv1_1_features],\n",
    "                                               stddev=0.1, dtype=tf.float32))\n",
    "conv1_1_bias = tf.Variable(tf.zeros([conv1_1_features], dtype=tf.float32))\n",
    "\n",
    "conv1_2_weight = tf.Variable(tf.truncated_normal([4, 4, conv1_1_features, conv1_2_features],\n",
    "                                               stddev=0.1, dtype=tf.float32))\n",
    "conv1_2_bias = tf.Variable(tf.zeros([conv1_2_features], dtype=tf.float32))\n",
    "\n",
    "conv2_1_weight = tf.Variable(tf.truncated_normal([4, 4, conv1_2_features,conv2_1_features],\n",
    "                                               stddev=0.1, dtype=tf.float32))\n",
    "conv2_1_bias = tf.Variable(tf.zeros([conv2_1_features], dtype=tf.float32))\n",
    "\n",
    "conv2_2_weight = tf.Variable(tf.truncated_normal([4, 4, conv2_1_features,conv2_2_features],\n",
    "                                               stddev=0.1, dtype=tf.float32))\n",
    "\n",
    "conv2_2_bias = tf.Variable(tf.zeros([conv2_2_features], dtype=tf.float32))\n",
    "\n",
    "conv3_1_weight = tf.Variable(tf.truncated_normal([4, 4, conv2_2_features,conv3_1_features],\n",
    "                                               stddev=0.1, dtype=tf.float32))\n",
    "conv3_1_bias = tf.Variable(tf.zeros([conv3_1_features], dtype=tf.float32))\n",
    "\n",
    "conv3_2_weight = tf.Variable(tf.truncated_normal([4, 4, conv3_1_features,conv3_2_features],\n",
    "                                               stddev=0.1, dtype=tf.float32))\n",
    "\n",
    "conv3_2_bias = tf.Variable(tf.zeros([conv3_2_features], dtype=tf.float32))\n",
    "\n",
    "\n",
    "\n",
    "Q_W1 = tf.Variable(xavier_init([X_dim/8/8*conv3_2_features, h_dim]))\n",
    "Q_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n",
    "\n",
    "Q_W2_mu = tf.Variable(xavier_init([h_dim, z_dim]))\n",
    "Q_b2_mu = tf.Variable(tf.zeros(shape=[z_dim]))\n",
    "\n",
    "Q_W2_sigma = tf.Variable(xavier_init([h_dim, z_dim]))\n",
    "Q_b2_sigma = tf.Variable(tf.zeros(shape=[z_dim]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================== P(X|z) ======================================\n",
    "P_W1 = tf.Variable(xavier_init([z_dim, h_dim]),name=\"P_W1\")\n",
    "P_b1 = tf.Variable(tf.zeros(shape=[h_dim]),name=\"P_b1\")\n",
    "\n",
    "deconv2_1_weight = tf.Variable(tf.truncated_normal([4, 4, deconv2_1_features, 1],\n",
    "                                               stddev=0.1, dtype=tf.float32))\n",
    "deconv2_1_bias = tf.Variable(tf.zeros([deconv2_1_features], dtype=tf.float32))\n",
    "\n",
    "deconv2_2_weight = tf.Variable(tf.truncated_normal([4, 4, deconv2_2_features,deconv2_1_features],\n",
    "                                               stddev=0.1, dtype=tf.float32))\n",
    "deconv2_2_bias = tf.Variable(tf.zeros([deconv2_2_features], dtype=tf.float32))\n",
    "\n",
    "deconv3_1_weight = tf.Variable(tf.truncated_normal([4, 4, deconv3_1_features, deconv2_2_features],\n",
    "                                               stddev=0.1, dtype=tf.float32))\n",
    "deconv3_1_bias = tf.Variable(tf.zeros([deconv3_1_features], dtype=tf.float32))\n",
    "\n",
    "deconv3_2_weight = tf.Variable(tf.truncated_normal([4, 4, deconv3_2_features, deconv3_1_features],\n",
    "                                               stddev=0.1, dtype=tf.float32))\n",
    "deconv3_2_bias = tf.Variable(tf.zeros([deconv3_2_features], dtype=tf.float32))\n",
    "\n",
    "deconv4_1_weight = tf.Variable(tf.truncated_normal([4, 4, deconv4_1_features, deconv3_2_features],\n",
    "                                               stddev=0.1, dtype=tf.float32))\n",
    "deconv4_1_bias = tf.Variable(tf.zeros([deconv4_1_features], dtype=tf.float32))\n",
    "\n",
    "deconv4_2_weight = tf.Variable(tf.truncated_normal([4, 4, deconv4_2_features, deconv4_1_features],\n",
    "                                               stddev=0.1, dtype=tf.float32))\n",
    "deconv4_2_bias = tf.Variable(tf.zeros([deconv4_2_features], dtype=tf.float32))\n",
    "\n",
    "deconv5_weight = tf.Variable(tf.truncated_normal([4, 4, 1, deconv4_2_features],\n",
    "                                               stddev=0.1, dtype=tf.float32))\n",
    "deconv5_bias = tf.Variable(tf.zeros([1], dtype=tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# =============================== Middel Results ====================================\n",
    "z_mu, z_logvar = Q(X)\n",
    "z_sample = sample_z(z_mu, z_logvar)\n",
    "prob = P(z_sample)\n",
    "\n",
    "# Sampling from random z\n",
    "X_samples_gen = P(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######## style transfer ########\n",
    "style_image = tf.placeholder(tf.float32, shape=(mb_size,height,width,3))\n",
    "\n",
    "# z_mu_style, z_logvar_style = Q(style_image[:,:,:,0])\n",
    "\n",
    "# z_sample_style = z_mu_style + tf.sqrt(tf.exp(z_logvar_style/2))*tf.random_normal([mb_size, z_dim])\n",
    "\n",
    "\n",
    "X_samples = P(z)\n",
    "\n",
    "combination_image_temp=tf.reshape(X_samples,[mb_size, height, width, 1])*1\n",
    "combination_image = tf.concat([combination_image_temp, combination_image_temp,combination_image_temp], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =============================== Middel Results ====================================\n",
    "# z_mu, z_logvar = Q(X)\n",
    "# z_sample = sample_z(z_mu, z_logvar)\n",
    "# prob = P(z_sample)\n",
    "\n",
    "# # Sampling from random z\n",
    "# X_samples = P(z)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######## style transfer ########\n",
    "# combination_image_temp=tf.reshape(X_samples,[mb_size, height, width, 1])*1\n",
    "# combination_image = tf.concat([combination_image_temp, combination_image_temp,combination_image_temp], 3)\n",
    "\n",
    "# style_image = tf.placeholder(tf.float32, shape=(mb_size,height,width,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### block 1 ########\n",
    "conv_out1 = conv2d(combination_image, W_conv1, stride=1, padding='SAME')\n",
    "conv_out1 = tf.nn.bias_add(conv_out1, b_conv1)\n",
    "conv_out1 = tf.nn.relu(conv_out1)\n",
    "\n",
    "conv_out2 = conv2d(conv_out1, W_conv2, stride=1, padding='SAME')\n",
    "conv_out2 = tf.nn.bias_add(conv_out2, b_conv2)\n",
    "conv_out2 = tf.nn.relu(conv_out2)\n",
    "conv_out2 = max_pool(conv_out2, k_size=2, stride=2, padding=\"SAME\")\n",
    "\n",
    "######### block 2 ########\n",
    "conv_out3 = conv2d(conv_out2, W_conv3, stride=1, padding='SAME')\n",
    "conv_out3 = tf.nn.bias_add(conv_out3, b_conv3)\n",
    "conv_out3 = tf.nn.relu(conv_out3)\n",
    "\n",
    "conv_out4 = conv2d(conv_out3, W_conv4, stride=1, padding='SAME')\n",
    "conv_out4 = tf.nn.bias_add(conv_out4, b_conv4)\n",
    "conv_out4 = tf.nn.relu(conv_out4)\n",
    "conv_out4 = max_pool(conv_out4, k_size=2, stride=2, padding=\"SAME\")\n",
    "\n",
    "######### block 3 ########\n",
    "conv_out5 = conv2d(conv_out4, W_conv5, stride=1, padding='SAME')\n",
    "conv_out5 = tf.nn.bias_add(conv_out5, b_conv5)\n",
    "conv_out5 = tf.nn.relu(conv_out5)\n",
    "\n",
    "conv_out6 = conv2d(conv_out5, W_conv6, stride=1, padding='SAME')\n",
    "conv_out6 = tf.nn.bias_add(conv_out6, b_conv6)\n",
    "conv_out6 = tf.nn.relu(conv_out6)\n",
    "\n",
    "conv_out7 = conv2d(conv_out6, W_conv7, stride=1, padding='SAME')\n",
    "conv_out7 = tf.nn.bias_add(conv_out7, b_conv7)\n",
    "conv_out7 = tf.nn.relu(conv_out7)\n",
    "conv_out7 = max_pool(conv_out7, k_size=2, stride=2, padding=\"SAME\")\n",
    "\n",
    "######### block 4 ########\n",
    "conv_out8 = conv2d(conv_out7, W_conv8, stride=1, padding='SAME')\n",
    "conv_out8 = tf.nn.bias_add(conv_out8, b_conv8)\n",
    "conv_out8 = tf.nn.relu(conv_out8)\n",
    "\n",
    "conv_out9 = conv2d(conv_out8, W_conv9, stride=1, padding='SAME')\n",
    "conv_out9 = tf.nn.bias_add(conv_out9, b_conv9)\n",
    "conv_out9 = tf.nn.relu(conv_out9)\n",
    "\n",
    "conv_out10= conv2d(conv_out9, W_conv10, stride=1, padding='SAME')\n",
    "conv_out10= tf.nn.bias_add(conv_out10, b_conv10)\n",
    "conv_out10= tf.nn.relu(conv_out10)\n",
    "conv_out10 = max_pool(conv_out10, k_size=2, stride=2, padding=\"SAME\")\n",
    "\n",
    "######### block 5 ########\n",
    "conv_out11= conv2d(conv_out10, W_conv11, stride=1, padding='SAME')\n",
    "conv_out11= tf.nn.bias_add(conv_out11, b_conv11)\n",
    "conv_out11= tf.nn.relu(conv_out11)\n",
    "\n",
    "conv_out12= conv2d(conv_out11, W_conv12, stride=1, padding='SAME')\n",
    "conv_out12= tf.nn.bias_add(conv_out12, b_conv12)\n",
    "conv_out12= tf.nn.relu(conv_out12)\n",
    "\n",
    "conv_out13= conv2d(conv_out12, W_conv13, stride=1, padding='SAME')\n",
    "conv_out13= tf.nn.bias_add(conv_out13, b_conv12)\n",
    "conv_out13= tf.nn.relu(conv_out13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### block 1 ########\n",
    "conv_out1_S = conv2d(style_image, W_conv1, stride=1, padding='SAME')\n",
    "conv_out1_S = tf.nn.bias_add(conv_out1_S, b_conv1)\n",
    "conv_out1_S = tf.nn.relu(conv_out1_S)\n",
    "\n",
    "conv_out2_S = conv2d(conv_out1_S, W_conv2, stride=1, padding='SAME')\n",
    "conv_out2_S = tf.nn.bias_add(conv_out2_S, b_conv2)\n",
    "conv_out2_S = tf.nn.relu(conv_out2_S)\n",
    "conv_out2_S = max_pool(conv_out2_S, k_size=2, stride=2, padding=\"SAME\")\n",
    "\n",
    "######### block 2 ########\n",
    "conv_out3_S = conv2d(conv_out2_S, W_conv3, stride=1, padding='SAME')\n",
    "conv_out3_S = tf.nn.bias_add(conv_out3_S, b_conv3)\n",
    "conv_out3_S = tf.nn.relu(conv_out3_S)\n",
    "\n",
    "conv_out4_S = conv2d(conv_out3_S, W_conv4, stride=1, padding='SAME')\n",
    "conv_out4_S = tf.nn.bias_add(conv_out4_S, b_conv4)\n",
    "conv_out4_S = tf.nn.relu(conv_out4_S)\n",
    "conv_out4_S = max_pool(conv_out4_S, k_size=2, stride=2, padding=\"SAME\")\n",
    "\n",
    "######### block 3 ########\n",
    "conv_out5_S = conv2d(conv_out4_S, W_conv5, stride=1, padding='SAME')\n",
    "conv_out5_S = tf.nn.bias_add(conv_out5_S, b_conv5)\n",
    "conv_out5_S = tf.nn.relu(conv_out5_S)\n",
    "\n",
    "conv_out6_S = conv2d(conv_out5_S, W_conv6, stride=1, padding='SAME')\n",
    "conv_out6_S = tf.nn.bias_add(conv_out6_S, b_conv6)\n",
    "conv_out6_S = tf.nn.relu(conv_out6_S)\n",
    "\n",
    "conv_out7_S = conv2d(conv_out6_S, W_conv7, stride=1, padding='SAME')\n",
    "conv_out7_S = tf.nn.bias_add(conv_out7_S, b_conv7)\n",
    "conv_out7_S = tf.nn.relu(conv_out7_S)\n",
    "conv_out7_S = max_pool(conv_out7_S, k_size=2, stride=2, padding=\"SAME\")\n",
    "\n",
    "######### block 4 ########\n",
    "conv_out8_S = conv2d(conv_out7_S, W_conv8, stride=1, padding='SAME')\n",
    "conv_out8_S = tf.nn.bias_add(conv_out8_S, b_conv8)\n",
    "conv_out8_S = tf.nn.relu(conv_out8_S)\n",
    "\n",
    "conv_out9_S = conv2d(conv_out8_S, W_conv9, stride=1, padding='SAME')\n",
    "conv_out9_S = tf.nn.bias_add(conv_out9_S, b_conv9)\n",
    "conv_out9_S = tf.nn.relu(conv_out9_S)\n",
    "\n",
    "conv_out10_S= conv2d(conv_out9_S, W_conv10, stride=1, padding='SAME')\n",
    "conv_out10_S= tf.nn.bias_add(conv_out10_S, b_conv10)\n",
    "conv_out10_S= tf.nn.relu(conv_out10_S)\n",
    "conv_out10_S= max_pool(conv_out10_S, k_size=2, stride=2, padding=\"SAME\")\n",
    "\n",
    "######### block 5 ########\n",
    "conv_out11_S= conv2d(conv_out10_S, W_conv11, stride=1, padding='SAME')\n",
    "conv_out11_S= tf.nn.bias_add(conv_out11_S, b_conv11)\n",
    "conv_out11_S= tf.nn.relu(conv_out11_S)\n",
    "\n",
    "conv_out12_S= conv2d(conv_out11_S, W_conv12, stride=1, padding='SAME')\n",
    "conv_out12_S= tf.nn.bias_add(conv_out12_S, b_conv12)\n",
    "conv_out12_S= tf.nn.relu(conv_out12_S)\n",
    "\n",
    "conv_out13_S= conv2d(conv_out12_S, W_conv13, stride=1, padding='SAME')\n",
    "conv_out13_S= tf.nn.bias_add(conv_out13_S, b_conv13)\n",
    "conv_out13_S= tf.nn.relu(conv_out13_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================== latent to Young's Module ====================================\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer with linear activation\n",
    "    out_layer = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([z_dim, z_dim/4])),\n",
    "    'h2': tf.Variable(tf.random_normal([z_dim/4, 1])),\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([z_dim/4])),\n",
    "    'b2': tf.Variable(tf.random_normal([1])),\n",
    "}\n",
    "\n",
    "label = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "pred = multilayer_perceptron(z_sample, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================== Cost ====================================\n",
    "\n",
    "# Style transfer\n",
    "sl1 = style_loss(conv_out2_S,conv_out2)\n",
    "sl2 = style_loss(conv_out4_S,conv_out4)\n",
    "sl3 = style_loss(conv_out1_S,conv_out1)\n",
    "sl4 = style_loss(conv_out3_S,conv_out3)\n",
    "sl = (sl1 + sl2 + sl3 + sl4)/1e0\n",
    "\n",
    "# Young's modulus cost\n",
    "# Young_cost = tf.reduce_mean(tf.square(pred - label))*0\n",
    "\n",
    "# claps cost\n",
    "cl1 = claps_loss(conv_out2)\n",
    "cl2 = claps_loss(conv_out4)\n",
    "cl3 = claps_loss(conv_out1)\n",
    "cl4 = claps_loss(conv_out3)\n",
    "cl = (cl1+cl2+cl3+cl4)*1e0\n",
    "\n",
    "# E[log P(X|z)]\n",
    "recon_loss = tf.reduce_mean(tf.reduce_sum(tf.square((tf.reshape(prob,[mb_size, X_dim])-\n",
    "                                                     tf.reshape(X,[mb_size, X_dim]))), 1))/1e0\n",
    "\n",
    "# D_KL(Q(z|X) || P(z|X)); calculate in closed form as both dist. are Gaussian\n",
    "\n",
    "kl_loss = tf.reduce_mean(0.5 * tf.reduce_sum(tf.exp(z_logvar) + z_mu**2 - 1. - z_logvar, 1))*kl_ratio\n",
    "\n",
    "# VAE loss\n",
    "# vae_loss = tf.reduce_mean(recon_loss + kl_loss + cl + Young_cost + sl)\n",
    "vae_loss = tf.reduce_mean(recon_loss + kl_loss + cl + sl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "solver = tf.train.AdamOptimizer(learning_rate=0.00001).minimize(vae_loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if not os.path.exists('out/'):\n",
    "    os.makedirs('out/')\n",
    "\n",
    "i = 0\n",
    "kl_ratio_initial=1e0\n",
    "PC_ori=[]\n",
    "saver = tf.train.Saver()\n",
    "cl_E_store=[]\n",
    "kl_E_store=[]\n",
    "for it in range(10000):\n",
    "    X_mb = images[(it*mb_size)%len(images):(it*mb_size)%len(images)+mb_size]\n",
    "#     Y_L = images_L[(it*mb_size)%len(images):(it*mb_size)%len(images)+mb_size]\n",
    "    random_index=random.sample(range(len(style_array)),  mb_size)\n",
    "    style_array_input = style_array[random_index]\n",
    "    \n",
    "#     print((it*10)%100,(it*10)%100+mb_size)\n",
    "    _, loss, recon_E, kl_E,cl_E, sl_E, sl_E1,sl_E2,sl_E3,sl_E4 = sess.run([solver, vae_loss, recon_loss, kl_loss,\n",
    "                                                                             cl, sl, sl1, sl2, sl3, sl4],\n",
    "                                                           feed_dict={X: X_mb.reshape(mb_size, width, height, 1),\n",
    "                                                           style_image: style_array_input.reshape(mb_size,height,width,3),\n",
    "                                                           z: np.random.randn(mb_size, z_dim),\n",
    "                                                           kl_ratio:kl_ratio_initial})\n",
    "\n",
    "    \n",
    "    if recon_E <=2e2:\n",
    "        kl_ratio_initial=5e1\n",
    "#     if recon_E <=2e3:\n",
    "#         kl_ratio_initial=2e1        \n",
    "        \n",
    "    cl_E_store.append(cl_E)\n",
    "    kl_E_store.append(kl_E)\n",
    "    if it % 100 == 0:\n",
    "        print('Iter: {}'.format(it))\n",
    "        print('Loss: {}'.format(loss))\n",
    "        print('recon_E:{}'.format(recon_E))\n",
    "        print('kl_E:{}'.format(kl_E))\n",
    "#         print('Young_E:{}'.format(Young_E))\n",
    "        print('cl_E:{}'.format(cl_E))\n",
    "        print('sl_E:{}'.format(sl_E))\n",
    "        print('sl_E1:{}'.format(sl_E1))\n",
    "        print('sl_E2:{}'.format(sl_E2))\n",
    "        print('sl_E3:{}'.format(sl_E3))\n",
    "        print('sl_E4:{}'.format(sl_E4))\n",
    "        \n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(np.log(kl_E_store))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = saver.save(sess, \"Hechao_Data/model_hechao_deeper_vali_model_adjusted.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# save_path = saver.save(sess, \"Hechao_Data/model_hechao_deeper.ckpt\")\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, \"Hechao_Data/model_hechao_deeper_vali_version2.ckpt\")\n",
    "\n",
    "for it in range(100000):\n",
    "    X_mb = images[(it*mb_size)%len(images):(it*mb_size)%len(images)+mb_size]\n",
    "    Y_L = images_L[(it*mb_size)%len(images):(it*mb_size)%len(images)+mb_size]\n",
    "    style_array_input = style_array[random.sample(range(len(style_array)),  mb_size)]\n",
    "    \n",
    "#     print((it*10)%100,(it*10)%100+mb_size)\n",
    "    _, loss, recon_E, kl_E,cl_E, sl_E,Young_E, sl_E1,sl_E2,sl_E3,sl_E4 = sess.run([solver, vae_loss, recon_loss, kl_loss,\n",
    "                                                                             cl, sl, Young_cost, sl1, sl2, sl3, sl4],\n",
    "                                                           feed_dict={X: X_mb.reshape(mb_size, width, height, 1),\n",
    "                                                           label:Y_L.reshape(mb_size,1),\n",
    "                                                           style_image: style_array_input.reshape(mb_size,128,128,3),\n",
    "                                                           z: np.random.randn(mb_size, z_dim)})\n",
    "\n",
    "    \n",
    "    \n",
    "    cl_E_store.append(cl_E)\n",
    "    if it % 200 == 0:\n",
    "        print('Iter: {}'.format(it))\n",
    "        print('Loss: {}'.format(loss))\n",
    "        print('recon_E:{}'.format(recon_E))\n",
    "        print('kl_E:{}'.format(kl_E))\n",
    "        print('Young_E:{}'.format(Young_E))\n",
    "        print('cl_E:{}'.format(cl_E))\n",
    "        print('sl_E:{}'.format(sl_E))\n",
    "        print('sl_E1:{}'.format(sl_E1))\n",
    "        print('sl_E2:{}'.format(sl_E2))\n",
    "        print('sl_E3:{}'.format(sl_E3))\n",
    "        print('sl_E4:{}'.format(sl_E4))\n",
    "        \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(20, 20))\n",
    "    gs = gridspec.GridSpec(1, 10)\n",
    "    gs.update(wspace=0.05, hspace=0.0)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "#         ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(40, 40), cmap='Greys_r')\n",
    "\n",
    "X_mb = images[0:mb_size]\n",
    "recon = sess.run(prob,feed_dict={X: X_mb.reshape(mb_size, width, height, 1)})\n",
    "plot(images[0:10])\n",
    "plot(recon[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import math\n",
    "%matplotlib inline\n",
    "\n",
    "test_mu,test_logvar=sess.run([z_mu, z_logvar],feed_dict={X: X_mb.reshape(mb_size, width, height, 1),\n",
    "#                                                            label:Y_L.reshape(mb_size,1),\n",
    "                                                           style_image: style_array_input.reshape(mb_size,40,40,3),\n",
    "                                                           z: np.random.randn(mb_size, z_dim)})\n",
    "\n",
    "mu = np.mean(test_mu)\n",
    "var= np.mean(np.exp(test_logvar))\n",
    "sigma = math.sqrt(var)\n",
    "x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n",
    "plt.plot(x,mlab.normpdf(x, mu, sigma),label='VAE')\n",
    "\n",
    "x = np.linspace(0 - 3*1, 0 + 3*1, 100)\n",
    "plt.plot(x,mlab.normpdf(x, 0, 1),label='Standard Norm')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sio.savemat('Hechao_Data/Reconstruction.mat',mdict={'recon':recon})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "X_mb = images[0:mb_size]\n",
    "latent_z = sess.run(z_sample,feed_dict={X: X_mb.reshape(mb_size, width, height, 1)})\n",
    "plt.hist(latent_z.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=np.random.randn(mb_size, z_dim)\n",
    "plt.hist(test.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Random Generation\n",
    "%matplotlib inline\n",
    "samples = abs(sess.run(X_samples_gen, feed_dict={z: np.random.randn(mb_size, z_dim)}))\n",
    "# samples[samples>0.5]=1\n",
    "# samples[samples<=0.5]=0\n",
    "plot(samples[0:10])\n",
    "plot(samples[10:20])\n",
    "samples = abs(1-sess.run(X_samples_gen, feed_dict={z: np.random.randn(mb_size, z_dim)}))\n",
    "# samples[samples>0.5]=1\n",
    "# samples[samples<=0.5]=0\n",
    "plot(samples[0:10])\n",
    "plot(samples[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####### Random Generation\n",
    "%matplotlib inline\n",
    "xtr=np.zeros([mb_size*50,width,height,1])\n",
    "samples = sess.run(X_samples_gen, feed_dict={z: np.random.randn(mb_size, z_dim)/1})\n",
    "\n",
    "k=0\n",
    "for i in range(50):  \n",
    "    samples = sess.run(X_samples_gen, feed_dict={z: np.random.randn(mb_size, z_dim)/1})\n",
    "    xtr[k*mb_size:k*mb_size+mb_size,:,:,:]=samples\n",
    "    k=k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sio.savemat('Hechao_Data/Random_Generation_T400V100T100_1000samples.mat',mdict={'samples':xtr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================== Validation(Samples that can't predict well) ====================================\n",
    "images_test_index = (sio.loadmat('Hechao_Data/large_error_validation_index.mat')['index'])+32\n",
    "images_test = images[images_test_index.reshape(-1)]\n",
    "\n",
    "X_mb_test=images_test[0:mb_size]\n",
    "z_temp=sess.run(z_mu, feed_dict={X:X_mb_test.reshape(mb_size, width, height, 1)})\n",
    "sample_temp=sess.run(X_samples, feed_dict={z: z_temp[0:mb_size]})\n",
    "# plot_new(sample_temp)\n",
    "plot(X_mb_test[0:10])\n",
    "plot(sample_temp[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[128+32].reshape(128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# =============================== Modification on Hard Samples ====================================\n",
    "# 90 index, we pick the first 80 and leave the rest 10 for future validation\n",
    "def plot_mid(samples):\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    gs = gridspec.GridSpec(3, mid_res)\n",
    "    gs.update(wspace=0.05, hspace=0.0)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "#         ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(128, 128), cmap='Greys_r')\n",
    "    \n",
    "def plot_ori(samples):\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    gs = gridspec.GridSpec(1, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.0)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "#         ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(128, 128), cmap='Greys_r')\n",
    "\n",
    "\n",
    "images_test_index = (sio.loadmat('Hechao_Data/large_error_validation_400T100V100T_26index_selected_version3.mat')\n",
    "                     ['index'])\n",
    "images_diff = images[images_test_index.reshape(-1)]\n",
    "\n",
    "mid_res=16 # number of generation between two samples\n",
    "\n",
    "\n",
    "z_store=np.zeros([(len(images_diff)-1)*mid_res,16])\n",
    "\n",
    "for ii in range(len(images_diff)-mb_size+1):\n",
    "    X_mb = images_diff[ii:ii+mb_size]\n",
    "    z_temp=sess.run(z_mu, feed_dict={X:X_mb.reshape(mb_size, width, height, 1)})\n",
    "    for i in range(mb_size-1):  \n",
    "        step=(z_temp[i+1]-z_temp[i])/(mid_res+1.)\n",
    "        for j in range(mid_res):\n",
    "            z_store_temp=z_temp[i]+step*(j+1)\n",
    "            z_store[(i+ii)*mid_res+j,:]=z_store_temp\n",
    "#             print((i+ii)*mid_res+j)\n",
    "\n",
    "##### show example #####\n",
    "z_store_test=np.array(z_store[0:mb_size])    \n",
    "sample_generation=sess.run(X_samples_gen, feed_dict={z: z_store_test})\n",
    "plot_ori(images_diff[0:mb_size].reshape(mb_size, width, height, 1)[0:4])\n",
    "plot_mid(sample_generation[0:3*mid_res])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(images[images_test_index.reshape(-1)][-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_store=[]\n",
    "# z_store=np.array(z_store) \n",
    "# for i in range(len(z_store)/mb_size):\n",
    "#     sample_generation=sess.run(X_samples, feed_dict={z: z_store[i*mb_size:(i+1)*mb_size,:]})\n",
    "#     sample_store.append(sample_generation)\n",
    "\n",
    "# sample_total=sample_store[0]\n",
    "# for i in range(1,len(sample_store)):\n",
    "#     sample_total=np.concatenate((sample_total, sample_store[i]), axis=0)\n",
    "    \n",
    "# sio.savemat('Hechao_Data/Generation_Interpo4_selected26_large_error_validation_based_T400V100T100_400samples.mat',\n",
    "#             mdict={'sample_total':sample_total})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_store=np.zeros([len(z_store),128,128,1])\n",
    "z_store=np.array(z_store) \n",
    "for i in range(len(z_store)-mb_size+1):\n",
    "    sample_generation=sess.run(X_samples_gen, feed_dict={z: z_store[i:i+mb_size,:]})\n",
    "    sample_store[i:i+mb_size,:,:,:]=sample_generation   \n",
    "    \n",
    "sio.savemat('Hechao_Data/Generation_newmodel_selected26index_large_error_interpo_T400V100T100_400samples.mat',\n",
    "            mdict={'sample_total':sample_store})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(sample_store[-10:])\n",
    "plot(sample_store[-20:-10])\n",
    "plot(sample_store[-30:-20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    gs = gridspec.GridSpec(1, 10)\n",
    "    gs.update(wspace=0.05, hspace=0.0)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "#         ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(128, 128), cmap='Greys_r')\n",
    "    \n",
    "plot(images_diff[0:10])\n",
    "plot(images_diff[10:20])\n",
    "plot(images_diff[20:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "images_hechao = np.array(sio.loadmat('Hechao_Data/Generation_test.mat')['sample_store'],dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================== Modification on Hard Samples ====================================\n",
    "# images_index = np.array(sio.loadmat('alloy_mat/diff_fig.mat')['index'],dtype='float32')\n",
    "\n",
    "images_test_index = (sio.loadmat('Hechao_Data/large_error_validation_index.mat')['index'])+32\n",
    "images_diff = images[images_test_index.reshape(-1)]\n",
    "\n",
    "X_mb = images_diff[0:mb_size]\n",
    "# recon = sess.run(prob,feed_dict={X: X_mb.reshape(mb_size, width, height, num_channels_1),\n",
    "#                                  label:Y_L.reshape(mb_size,1)})\n",
    "\n",
    "xtr=np.zeros([mb_size*10,width,height,1])\n",
    "k=0\n",
    "for i in range(10):  \n",
    "    z_temp=sess.run(z_mu, feed_dict={X:X_mb.reshape(mb_size, width, height, 1)})\n",
    "    \n",
    "    rand_index=random.sample(np.arange(16),16)\n",
    "    z_temp[:,rand_index]=z_temp[:,rand_index]+random.random()/1 \n",
    "    \n",
    "    sample_temp=sess.run(X_samples, feed_dict={z: z_temp[0:mb_size]})\n",
    "    xtr[k*mb_size:k*mb_size+mb_size,:,:,:]=sample_temp\n",
    "    k=k+1\n",
    "    \n",
    "plot(X_mb[0:10])\n",
    "plot(xtr[0:10])\n",
    "plot(xtr[20:30])\n",
    "plot(xtr[40:50])\n",
    "plot(xtr[60:70])\n",
    "plot(xtr[80:90])\n",
    "plot(xtr[100:110])\n",
    "plot(xtr[120:130])\n",
    "plot(xtr[140:150])\n",
    "plot(xtr[160:170])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
